{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce17d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c804968e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download(\"stopwords\", quiet = True)\n",
    "nltk.download(\"wordnet\", quiet = True)\n",
    "nltk.download(\"punkt\", quiet = True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d93c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CricinfoScraper:\n",
    "    def __init__(self, url, base_url = None):\n",
    "        self.url = url\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def get_articles(self, start_page = 1, num_pages = 1):\n",
    "        articles = []\n",
    "        for page_num in range(start_page, start_page + num_pages + 1):\n",
    "            new_url = f\"{self.url}?page={page_num}\"\n",
    "            response = requests.get(new_url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            page_articles = self.scrape_page(soup)\n",
    "            articles.extend(page_articles)\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def scrape_page(self, soup):\n",
    "        articles = []\n",
    "        titles = []\n",
    "        urls = []\n",
    "        summaries = []\n",
    "        dates = []\n",
    "        \n",
    "        # getting title and link\n",
    "        for article in soup.find_all('h2', class_='ds-text-title-s ds-font-bold ds-text-typo'):\n",
    "            title = article.text.strip()\n",
    "            titles.append(title)\n",
    "        \n",
    "            link_tag = article.find_parent('a')\n",
    "            link = link_tag['href'] if link_tag else \"\"\n",
    "            if self.base_url:\n",
    "                link = self.base_url + link\n",
    "            urls.append(link)\n",
    "            \n",
    "        # getting summaries\n",
    "        for article in soup.find_all('p', class_='ds-text-compact-s ds-text-typo-mid2 ds-mt-1'):\n",
    "            summary = article.text.strip()\n",
    "            summaries.append(summary)\n",
    "            \n",
    "        # getting publication date\n",
    "        for article in soup.find_all('div', class_='ds-leading-[0] ds-text-typo-mid3 ds-mt-1'):\n",
    "            date_text = article.text.strip()\n",
    "            date = date_text.split('â€¢')[0]\n",
    "            dates.append(date)\n",
    "            \n",
    "        for title, url, summary, date in zip(titles, urls, summaries, dates):\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'link': url,\n",
    "                'summary': summary,\n",
    "                'date': date\n",
    "            })\n",
    "\n",
    "        return articles\n",
    "    \n",
    "    def save_articles(self, articles, filename):\n",
    "        df = pd.DataFrame(articles)\n",
    "        df.to_csv(filename, index = False)\n",
    "    \n",
    "    def load_and_combine_articles(self, filenames):\n",
    "        dfs = [pd.read_csv(filename) for filename in filenames]\n",
    "        df = pd.concat(dfs).drop_duplicates().dropna().reset_index(drop = True)\n",
    "        \n",
    "        # converting all text to lower case\n",
    "        df['title'] = df['title'].str.lower()\n",
    "        df['summary'] = df['summary'].str.lower()\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad47259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.english_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"'s(\\s|$)\", r'\\1', text)\n",
    "        text = text.replace(\"'\", \"\")\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, cleaned_text):\n",
    "        tokens = nltk.word_tokenize(cleaned_text)\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            split_token = re.split(r'[^0-9a-zA-Z]+', token)\n",
    "            split_token = [token for token in split_token if token]\n",
    "            new_tokens.extend(split_token)\n",
    "        return new_tokens\n",
    "\n",
    "    def lemmatize(self, tokens, stopwords = {}):\n",
    "        lemmatized_tokens = []\n",
    "        for token in tokens:\n",
    "            tag = nltk.pos_tag([token])[0][1]\n",
    "            if tag.startswith('J'):\n",
    "                tag = wordnet.ADJ\n",
    "            elif tag.startswith('V'):\n",
    "                tag = wordnet.VERB\n",
    "            elif tag.startswith('R'):\n",
    "                tag = wordnet.ADV\n",
    "            else:\n",
    "                tag = wordnet.NOUN\n",
    "\n",
    "            lemmatized = self.lemmatizer.lemmatize(token, pos=tag)\n",
    "            if (lemmatized not in stopwords) and (len(lemmatized) >= 2):\n",
    "                lemmatized_tokens.append(lemmatized)\n",
    "        return lemmatized_tokens\n",
    "    \n",
    "    def preprocess_text(self, text, stopwords={}):\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned_text)\n",
    "        return self.lemmatize(tokens, stopwords)\n",
    "    \n",
    "    def join_tokens(self, tokens):\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess_dataframe(self, df, text_columns):\n",
    "        for column in text_columns:\n",
    "            df[column] = df[column].apply(lambda x: self.preprocess_text(x, self.english_stopwords))\n",
    "            df[column] = df[column].apply(self.join_tokens)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00c6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.dummy = lambda x : x\n",
    "            \n",
    "    def count_vectorizer(self, text):\n",
    "        vectorizer = CountVectorizer(analyzer = str.split, tokenizer = str.split, preprocessor = self.dummy)\n",
    "        tf_text = vectorizer.fit_transform(text)\n",
    "        features = vectorizer.get_feature_names_out().tolist()\n",
    "        return tf_text, features\n",
    "    \n",
    "    def tfidf_vectorizer(self, text):\n",
    "        vectorizer = TfidfVectorizer(analyzer = str.split, tokenizer = str.split, preprocessor = self.dummy)\n",
    "        tf_text = vectorizer.fit_transform(text)\n",
    "        features = vectorizer.get_feature_names_out().tolist()\n",
    "        return tf_text, features\n",
    "    \n",
    "    def top_words_by_topic(self, text, n_topics = 10, n_top_words = 20, seed = 42):\n",
    "        tf_text, features = self.count_vectorizer(text)\n",
    "        lda = LatentDirichletAllocation(n_components = n_topics, random_state = seed, learning_method = 'online')\n",
    "        lda.fit(tf_text)\n",
    "        topics = lda.components_\n",
    "\n",
    "        top_words = []\n",
    "        for i in range(n_topics):\n",
    "            indices = topics[i].argsort()\n",
    "            top_indices = indices[-n_top_words : ]\n",
    "            words = [features[j] for j in top_indices]\n",
    "            top_words.append(words)\n",
    "        return top_words\n",
    "    \n",
    "    def extract_features(self, df):\n",
    "        tokenized_df = df.copy()\n",
    "\n",
    "        tf_title, features_title_tf = self.count_vectorizer(tokenized_df['title'])\n",
    "        tf_summary, features_summary_tf = self.count_vectorizer(tokenized_df['summary'])\n",
    "\n",
    "        tfidf_title, features_title_tfidf = self.tfidf_vectorizer(tokenized_df['title'])\n",
    "        tfidf_summary, features_summary_tfidf = self.tfidf_vectorizer(tokenized_df['summary'])\n",
    "\n",
    "        tfidf_title_dense = tfidf_title.toarray()\n",
    "        tfidf_summary_dense = tfidf_summary.toarray()\n",
    "\n",
    "        df_title_tfidf = pd.DataFrame(tfidf_title_dense, columns = features_title_tfidf)\n",
    "        df_summary_tfidf = pd.DataFrame(tfidf_summary_dense, columns = features_summary_tfidf)\n",
    "\n",
    "        vectorized_df = pd.concat([df_title_tfidf, df_summary_tfidf], axis = 1)\n",
    "\n",
    "        vectorized_df['original_title'] = df['title']\n",
    "        vectorized_df['original_summary'] = df['summary']\n",
    "        vectorized_df['link'] = df['link']\n",
    "\n",
    "        np.save('vectorized_df.npy', vectorized_df)\n",
    "\n",
    "        return vectorized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae3056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(self, vectorized_df):\n",
    "        self.vectorized_df = vectorized_df\n",
    "        self.numerical_df = vectorized_df.drop(['link', 'original_title', 'original_summary'], axis = 1)\n",
    "        self.cosine_sim = None\n",
    "        self.top_20_recs = None\n",
    "        \n",
    "    def compute_cosine_similarity(self):\n",
    "        self.cosine_sim = cosine_similarity(self.numerical_df)\n",
    "        np.save('cosine_sim.npy', self.cosine_sim)\n",
    "        return self.cosine_sim\n",
    "    \n",
    "    def load_cosine_similarity(self, file_path):\n",
    "        self.cosine_sim = np.load(file_path)\n",
    "        return self.cosine_sim\n",
    "    \n",
    "    def get_recommendations(self, article_id, top_n = 5):\n",
    "        if self.cosine_sim is None:\n",
    "            raise ValueError(\"Cosine similarity matrix not computed or loaded.\")\n",
    "        \n",
    "        scores = list(enumerate(self.cosine_sim[article_id]))\n",
    "        scores = sorted(scores, key=lambda x: x[1], reverse = True)\n",
    "        scores = scores[1 : top_n + 1]\n",
    "        article_indices = [i[0] for i in scores]\n",
    "        return article_indices\n",
    "    \n",
    "    def cache_top_recommendations(self, top_n = 20):\n",
    "        if self.cosine_sim is None:\n",
    "            raise ValueError(\"Cosine similarity matrix not computed or loaded.\")\n",
    "        \n",
    "        n = self.cosine_sim.shape[0]\n",
    "        self.top_20_recs = np.zeros((n, top_n), dtype = int)\n",
    "\n",
    "        for i in range(n):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processing article {i}\")\n",
    "            self.top_20_recs[i] = self.get_recommendations(i, top_n = top_n)\n",
    "\n",
    "        np.save('top_20_recommendations.npy', self.top_20_recs)\n",
    "        return self.top_20_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.espncricinfo.com/ci/content/story/news.html'\n",
    "base_url = 'https://www.espncricinfo.com'\n",
    "scraper = CricinfoScraper(url, base_url)\n",
    "\n",
    "articles = scraper.get_articles(1, 1000)\n",
    "scraper.save_articles(articles, 'cricinfo_articles.csv')\n",
    "\n",
    "more_articles = scraper.get_articles(1001, 300)\n",
    "scraper.save_articles(more_articles, 'cricinfo_articles_2.csv')\n",
    "\n",
    "articles_3 = scraper.get_articles(1301, 200)\n",
    "scraper.save_articles(articles_3, 'cricinfo_articles_3.csv')\n",
    "\n",
    "filenames = ['cricinfo_articles.csv', 'cricinfo_articles_2.csv', 'cricinfo_articles_3.csv']\n",
    "df = scraper.load_and_combine_articles(filenames)\n",
    "df.to_csv('articles_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('articles_full.csv')\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "text_columns = ['title', 'summary']\n",
    "df = preprocessor.preprocess_dataframe(df, text_columns)\n",
    "df.to_csv('tokenized_articles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = pd.read_csv('tokenized_articles.csv')\n",
    "\n",
    "extractor = FeatureExtractor()\n",
    "vectorized_df = extractor.extract_features(tokenized_df)\n",
    "\n",
    "top_words_title = extractor.top_words_by_topic(tokenized_df['title'])\n",
    "top_words_summary = extractor.top_words_by_topic(tokenized_df['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df = np.load('vectorized_df.npy', allow_pickle = True).item()\n",
    "recommender = Recommender(vectorized_df)\n",
    "\n",
    "# Compute and save cosine similarity\n",
    "cosine_sim = recommender.compute_cosine_similarity()\n",
    "\n",
    "# Cache the top 20 recommendations for each article\n",
    "top_20_recs = recommender.cache_top_recommendations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
